{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5279c2",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e26a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "import torch\n",
    "from utils_cv.action_recognition.dataset import VideoDataset\n",
    "from utils_cv.action_recognition.model import VideoLearner \n",
    "from utils_cv.common.data import data_path\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0652970",
   "metadata": {},
   "source": [
    "# Load the pretrained model - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ced765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data\n",
    "DATA_PATH = data_path() / \"./kinetics700_2020/\"\n",
    "\n",
    "# Number of consecutive frames used as input to the DNN. Use: 32 for high accuracy, 8 for inference speed.\n",
    "MODEL_INPUT_SIZE = 8\n",
    "\n",
    "# Batch size. Reduce if running out of memory.\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347b43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = VideoDataset(DATA_PATH, batch_size=BATCH_SIZE, sample_length=MODEL_INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d6cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading r2plus1d_34_8_ig65m model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yangze2065/.cache/torch/hub/moabitcoin_ig65m-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "actionModel = VideoLearner(data, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3695e3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<utils_cv.action_recognition.model.VideoLearner object at 0x7fe1397bb550>\n"
     ]
    }
   ],
   "source": [
    "actionModel.load(model_name = \"R21D8_sharpening_knives\")\n",
    "print(actionModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8642d",
   "metadata": {},
   "source": [
    "# Loading unknown videos for action prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985ede06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Youtube Videos for Testing = 2 videos\n"
     ]
    }
   ],
   "source": [
    "# Set up input and output video path\n",
    "video_file_path = data_path()/\"video_clips/\"\n",
    "predicted_file_path = data_path()/\"video_clips/predicted/\"\n",
    "print(\"Number of Youtube Videos for Testing = \" + str(len(os.listdir(video_file_path))) + \" videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14677256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load video in sequence\n",
    "for video in os.listdir(video_file_path):\n",
    "    if os.path.isfile(os.path.join(video_file_path, video)):\n",
    "        os.environ['inputFile'] = os.path.join(video_file_path, video)\n",
    "        video_duration = !ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$inputFile\"\n",
    "        # Count the whole video duration\n",
    "        for time in video_duration:\n",
    "            time = int(float(time))\n",
    "            \n",
    "        # Parse the video for trimming and action recognition \n",
    "        for i in range(1, time-2, 2): \n",
    "            print(\"video name: \" + video)\n",
    "            print(\"start time at \" , i , \"second\")\n",
    "            print(\"end time at \" , i+2 , \"second\")\n",
    "            # Extract Youtube video ID\n",
    "            YoutubeID = os.path.splitext(video)[0]\n",
    "            os.environ['start'] = str(i)\n",
    "            # trim video into temporary 2 second clip \"temp-TRIM.mp4\"\n",
    "            !ffmpeg -hide_banner -loglevel error -i \"$inputFile\" -ss \"$start\" -t 2 -c:a copy \"./data/video_clips/temp-TRIM.mp4\"\n",
    "            # parse the 2s \"temp-TRIM.mp4\" into model for action detection\n",
    "            top_result = actionModel.predict_video(\"./data/video_clips/temp-TRIM.mp4\")\n",
    "            # remove the trimmed clip for next iteration\n",
    "            os.remove(\"./data/video_clips/temp-TRIM.mp4\")\n",
    "            clear_output()\n",
    "\n",
    "            # pull out the list from top 5 results and write JSON file\n",
    "            for predict_label, test_acc in top_result:\n",
    "                # Only pick those results that is detected for target action with confidance greater than 0.8\n",
    "                if predict_label == \"sharpening_knives\" and test_acc >= 0.8:\n",
    "                    video_data=[\n",
    "                        {\n",
    "                            \"videoId\": str(YoutubeID),\n",
    "                            \"type\": \"segment\",\n",
    "                            \"startTime\": float(i),\n",
    "                            \"endTime\": float(i+2),\n",
    "                            \"observer\": \"CSCE636-Spring2021-CochiLocoYang-v10\",\n",
    "                            \"isHuman\": \"false\",\n",
    "                            \"confirmedBySomeone\": \"false\",\n",
    "                            \"rejectedBySomeone\": \"false\",\n",
    "                            \"observation\": {\n",
    "                                \"label\": predict_label,\n",
    "                                \"labelConfidence\":test_acc\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                    \n",
    "                with open('CSCE636-Spring2021-CochiLocoYang-v10_unsorted.json', 'a') as video_json_file:\n",
    "                    json.dump(video_data, video_json_file)\n",
    "            \n",
    "        # Move predicted video to another folder\n",
    "        os.rename(os.path.join(video_file_path, video), os.path.join(predicted_file_path, video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c12ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "f378bc374f19337cff4ad47d4a39d321cfae98a9ff072b8d2ad773ca7dea7c2f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
